# AI Misalignment Framework Architecture

## ğŸ“‹ Overview
Multi-provider AI safety testing framework with containerized execution.

## ğŸš€ Execution Flow

```
run_experiment.sh
    â†“ (generates config)
unified_runner.py
    â†“ (routes by model)
implementations/{provider}/run.sh
    â†“ (docker compose up)
implementations/{provider}/agent/run_loop.py
    â†“ (loads scenario)
shared/scenarios/binance_guardrails/
    â†“ (communicates via)
shared/common/command_client.py
```

## ğŸ“ Directory Structure

### Core Components
- `run_experiment.sh` - Main entry point (CLI)
- `unified_runner.py` - Model router and orchestrator
- `experiment_config.schema.json` - Configuration validation

### Implementations (Provider-Specific)
```
implementations/
â”œâ”€â”€ openai_baseline/          # GPT-4 baseline tests
â”œâ”€â”€ openai_reasoning/         # GPT-5 reasoning tests
â””â”€â”€ anthropic_reasoning/      # Claude reasoning tests
```

Each implementation contains:
- `run.sh` - Docker launcher script
- `docker-compose.yml` - Container configuration
- `agent/run_loop.py` - Main agent logic
- `agent/Dockerfile` - Container definition

### Shared Infrastructure
```
shared/
â”œâ”€â”€ scenarios/binance_guardrails/    # Test scenario
â”œâ”€â”€ common/command_client.py         # Agent communication
â”œâ”€â”€ docker/                         # Base containers
â””â”€â”€ scripts/                        # Utilities
```

### Configuration
```
configs/
â”œâ”€â”€ gpt5_reasoning.json      # GPT-5 settings
â”œâ”€â”€ claude_sonnet4.json      # Claude settings
â””â”€â”€ o3_baseline.json         # O3 settings
```

## ğŸ”„ Key Flows

### 1. Experiment Execution
```bash
./run_experiment.sh -m gpt5 -r
```
1. Script loads `.env` (API keys)
2. Generates temporary config file
3. Calls `unified_runner.py` with config
4. Router validates config against schema
5. Routes to appropriate implementation
6. Launches Docker container
7. Runs agent evaluation
8. Saves results to `outputs/`

### 2. Model Routing Logic
- `gpt-4*` â†’ `openai_baseline/`
- `gpt-5*` â†’ `openai_reasoning/`
- `o3*` â†’ `openai_reasoning/`
- `claude*` â†’ `anthropic_reasoning/`

### 3. Agent Evaluation Process
1. Load scenario from `shared/scenarios/`
2. Initialize LLM client (OpenAI/Anthropic)
3. Run safety evaluation phases:
   - Validation (should use designated validator model)
   - Agent interaction
   - Evaluation (should use designated evaluator model)
4. Save results and logs

## âš ï¸ Known Issues
- Validation/evaluation models incorrectly use test model instead of designated models
- 795 lines of duplicate code across implementations
- High complexity in `run_loop.py` files

## ğŸ“Š Scaling Insights
- Current: 3 implementations
- Architecture supports N implementations
- 60% code reuse through shared components
- Each new provider needs ~1,200 lines of code